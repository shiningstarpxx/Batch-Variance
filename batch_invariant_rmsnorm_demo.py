#!/usr/bin/env python3
"""
Batch-invariant RMSNormæ¼”ç¤º

æ ¹æ®Thinking Machinesçš„blogï¼Œæ¼”ç¤ºRMSNormå¦‚ä½•ä»batch-variantå˜æˆbatch-invariant
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any
import time

# å¯¼å…¥å­—ä½“é…ç½®
try:
    from src.font_config import setup_chinese_fonts
except ImportError:
    from font_config import setup_chinese_fonts

setup_chinese_fonts()

class BatchInvariantRMSNormDemo:
    """Batch-invariant RMSNormæ¼”ç¤ºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤º"""
        self.results = {}
        
    def create_test_data(self, batch_sizes: List[int], seq_len: int = 512, hidden_dim: int = 1024) -> Dict[int, torch.Tensor]:
        """åˆ›å»ºä¸åŒæ‰¹å¤„ç†å¤§å°çš„æµ‹è¯•æ•°æ®"""
        data = {}
        torch.manual_seed(42)  # å›ºå®šç§å­ç¡®ä¿å¯é‡ç°æ€§
        
        for batch_size in batch_sizes:
            # åˆ›å»ºç›¸åŒçš„è¾“å…¥æ•°æ®ï¼Œåªæ˜¯æ‰¹å¤„ç†å¤§å°ä¸åŒ
            # æ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ç›¸åŒçš„ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿæ‰¹å¤„ç†å¤§å°å¯¹ç»“æœçš„å½±å“
            base_sample = torch.randn(seq_len, hidden_dim)
            batch_data = base_sample.unsqueeze(0).repeat(batch_size, 1, 1)
            data[batch_size] = batch_data
            
        return data
    
    def standard_rmsnorm(self, x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
        """æ ‡å‡†RMSNormå®ç° - Batch-variant"""
        # è®¡ç®—RMS (Root Mean Square)
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + eps)
        # å½’ä¸€åŒ–
        return x / rms
    
    def batch_variant_rmsnorm(self, x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
        """Batch-variant RMSNorm - æ¨¡æ‹ŸGPUå¹¶è¡Œå½’çº¦çš„éç¡®å®šæ€§"""
        batch_size, seq_len, hidden_dim = x.shape
        
        # æ¨¡æ‹ŸGPUå¹¶è¡Œå½’çº¦çš„éç¡®å®šæ€§
        # åœ¨å®é™…GPUå®ç°ä¸­ï¼Œå½’çº¦çš„é¡ºåºå¯èƒ½å› ä¸ºå¹¶è¡Œæ‰§è¡Œè€Œä¸åŒ
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        
        # æ¨¡æ‹Ÿä¸åŒçš„å½’çº¦ç­–ç•¥
        if batch_size == 1:
            # å•æ ·æœ¬ï¼šé¡ºåºå½’çº¦
            for i in range(hidden_dim):
                rms_squared += x[:, :, i:i+1] ** 2
        elif batch_size == 2:
            # åŒæ ·æœ¬ï¼šä¸¤ä¸¤å½’çº¦
            for i in range(0, hidden_dim, 2):
                if i + 1 < hidden_dim:
                    rms_squared += x[:, :, i:i+1] ** 2 + x[:, :, i+1:i+2] ** 2
                else:
                    rms_squared += x[:, :, i:i+1] ** 2
        elif batch_size == 4:
            # å››æ ·æœ¬ï¼šå››å››å½’çº¦
            for i in range(0, hidden_dim, 4):
                chunk_sum = torch.zeros_like(rms_squared)
                for j in range(min(4, hidden_dim - i)):
                    chunk_sum += x[:, :, i+j:i+j+1] ** 2
                rms_squared += chunk_sum
        else:
            # å…¶ä»–æƒ…å†µï¼šéšæœºå½’çº¦é¡ºåº
            indices = torch.randperm(hidden_dim)
            for i in indices:
                rms_squared += x[:, :, i:i+1] ** 2
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def batch_invariant_rmsnorm(self, x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
        """Batch-invariant RMSNorm - å›ºå®šå½’çº¦é¡ºåº"""
        batch_size, seq_len, hidden_dim = x.shape
        
        # ä½¿ç”¨å›ºå®šçš„å½’çº¦é¡ºåºï¼Œç¡®ä¿batch-invariant
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        
        # å›ºå®šé¡ºåºï¼šæ€»æ˜¯æŒ‰ç´¢å¼•é¡ºåºè¿›è¡Œå½’çº¦
        for i in range(hidden_dim):
            rms_squared += x[:, :, i:i+1] ** 2
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def demonstrate_batch_variance(self) -> None:
        """æ¼”ç¤ºBatch-varianceé—®é¢˜"""
        print("=== Batch-variance RMSNormæ¼”ç¤º ===\n")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_sizes = [1, 2, 4, 8]
        seq_len, hidden_dim = 256, 512
        
        print(f"ğŸ“Š æµ‹è¯•å‚æ•°:")
        print(f"   åºåˆ—é•¿åº¦: {seq_len}")
        print(f"   éšè—ç»´åº¦: {hidden_dim}")
        print(f"   æ‰¹å¤„ç†å¤§å°: {batch_sizes}")
        print()
        
        # åˆ›å»ºç›¸åŒçš„è¾“å…¥æ•°æ®
        base_input = torch.randn(seq_len, hidden_dim)
        
        results = {}
        
        for batch_size in batch_sizes:
            print(f"ğŸ”§ æµ‹è¯•æ‰¹å¤„ç†å¤§å°: {batch_size}")
            
            # åˆ›å»ºæ‰¹å¤„ç†æ•°æ®ï¼ˆæ‰€æœ‰æ ·æœ¬éƒ½ç›¸åŒï¼‰
            batch_input = base_input.unsqueeze(0).repeat(batch_size, 1, 1)
            
            # æ ‡å‡†RMSNorm
            std_output = self.standard_rmsnorm(batch_input)
            
            # Batch-variant RMSNorm
            variant_output = self.batch_variant_rmsnorm(batch_input)
            
            # Batch-invariant RMSNorm
            invariant_output = self.batch_invariant_rmsnorm(batch_input)
            
            # è®¡ç®—å·®å¼‚
            std_variant_diff = torch.max(torch.abs(std_output - variant_output)).item()
            std_invariant_diff = torch.max(torch.abs(std_output - invariant_output)).item()
            variant_invariant_diff = torch.max(torch.abs(variant_output - invariant_output)).item()
            
            results[batch_size] = {
                'std_variant_diff': std_variant_diff,
                'std_invariant_diff': std_invariant_diff,
                'variant_invariant_diff': variant_invariant_diff,
                'std_output': std_output[0].detach().cpu().numpy(),  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                'variant_output': variant_output[0].detach().cpu().numpy(),
                'invariant_output': invariant_output[0].detach().cpu().numpy()
            }
            
            print(f"   æ ‡å‡† vs Batch-variant: {std_variant_diff:.2e}")
            print(f"   æ ‡å‡† vs Batch-invariant: {std_invariant_diff:.2e}")
            print(f"   Batch-variant vs Batch-invariant: {variant_invariant_diff:.2e}")
            print()
        
        self.results = results
        return results
    
    def analyze_batch_invariance(self) -> None:
        """åˆ†æBatch-invariance"""
        print("=== Batch-invarianceåˆ†æ ===\n")
        
        if not self.results:
            print("è¯·å…ˆè¿è¡Œ demonstrate_batch_variance()")
            return
        
        print("ğŸ” å…³é”®è§‚å¯Ÿ:")
        print("1. **Batch-variant RMSNorm**: ä¸åŒæ‰¹å¤„ç†å¤§å°äº§ç”Ÿä¸åŒç»“æœ")
        print("2. **Batch-invariant RMSNorm**: ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒç»“æœï¼Œæ— è®ºæ‰¹å¤„ç†å¤§å°")
        print("3. **å·®å¼‚æ¥æº**: å¹¶è¡Œå½’çº¦çš„é¡ºåºä¸åŒ")
        print()
        
        # åˆ†æç¬¬ä¸€ä¸ªæ ·æœ¬åœ¨ä¸åŒæ‰¹å¤„ç†å¤§å°ä¸‹çš„ç»“æœ
        print("ğŸ“ˆ ç¬¬ä¸€ä¸ªæ ·æœ¬åœ¨ä¸åŒæ‰¹å¤„ç†å¤§å°ä¸‹çš„ç»“æœ:")
        for batch_size, result in self.results.items():
            first_sample_std = result['std_output']
            first_sample_variant = result['variant_output']
            first_sample_invariant = result['invariant_output']
            
            print(f"   æ‰¹å¤„ç†å¤§å° {batch_size}:")
            print(f"     æ ‡å‡†è¾“å‡ºå‰5ä¸ªå€¼: {first_sample_std[0, :5]}")
            print(f"     Variantè¾“å‡ºå‰5ä¸ªå€¼: {first_sample_variant[0, :5]}")
            print(f"     Invariantè¾“å‡ºå‰5ä¸ªå€¼: {first_sample_invariant[0, :5]}")
            print()
    
    def visualize_batch_variance(self) -> None:
        """å¯è§†åŒ–Batch-variance"""
        if not self.results:
            print("è¯·å…ˆè¿è¡Œ demonstrate_batch_variance()")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. å·®å¼‚å¯¹æ¯”
        ax1 = axes[0, 0]
        batch_sizes = list(self.results.keys())
        std_variant_diffs = [self.results[bs]['std_variant_diff'] for bs in batch_sizes]
        std_invariant_diffs = [self.results[bs]['std_invariant_diff'] for bs in batch_sizes]
        variant_invariant_diffs = [self.results[bs]['variant_invariant_diff'] for bs in batch_sizes]
        
        x = np.arange(len(batch_sizes))
        width = 0.25
        
        ax1.bar(x - width, std_variant_diffs, width, label='æ ‡å‡† vs Variant', alpha=0.8)
        ax1.bar(x, std_invariant_diffs, width, label='æ ‡å‡† vs Invariant', alpha=0.8)
        ax1.bar(x + width, variant_invariant_diffs, width, label='Variant vs Invariant', alpha=0.8)
        
        ax1.set_xlabel('æ‰¹å¤„ç†å¤§å°', fontsize=12)
        ax1.set_ylabel('æœ€å¤§å·®å¼‚', fontsize=12)
        ax1.set_title('RMSNormè¾“å‡ºå·®å¼‚å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_xticks(x)
        ax1.set_xticklabels(batch_sizes)
        ax1.set_yscale('log')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. è¾“å‡ºåˆ†å¸ƒå¯¹æ¯”
        ax2 = axes[0, 1]
        batch_size = 4  # é€‰æ‹©ä¸€ä¸ªæ‰¹å¤„ç†å¤§å°è¿›è¡Œè¯¦ç»†åˆ†æ
        result = self.results[batch_size]
        
        std_output = result['std_output'].flatten()
        variant_output = result['variant_output'].flatten()
        invariant_output = result['invariant_output'].flatten()
        
        ax2.hist(std_output, bins=50, alpha=0.5, label='æ ‡å‡†RMSNorm', density=True)
        ax2.hist(variant_output, bins=50, alpha=0.5, label='Batch-variant', density=True)
        ax2.hist(invariant_output, bins=50, alpha=0.5, label='Batch-invariant', density=True)
        
        ax2.set_xlabel('è¾“å‡ºå€¼', fontsize=12)
        ax2.set_ylabel('å¯†åº¦', fontsize=12)
        ax2.set_title(f'æ‰¹å¤„ç†å¤§å°{batch_size}çš„è¾“å‡ºåˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å·®å¼‚çƒ­å›¾
        ax3 = axes[1, 0]
        batch_size = 4
        result = self.results[batch_size]
        
        # è®¡ç®—å·®å¼‚çŸ©é˜µ
        std_variant_diff = np.abs(result['std_output'] - result['variant_output'])
        std_invariant_diff = np.abs(result['std_output'] - result['invariant_output'])
        
        # æ˜¾ç¤ºå‰50x50çš„å·®å¼‚
        im1 = ax3.imshow(std_variant_diff[:50, :50], cmap='Reds', aspect='auto')
        ax3.set_title('æ ‡å‡† vs Batch-variantå·®å¼‚çƒ­å›¾', fontsize=14, fontweight='bold')
        ax3.set_xlabel('éšè—ç»´åº¦', fontsize=12)
        ax3.set_ylabel('åºåˆ—ä½ç½®', fontsize=12)
        plt.colorbar(im1, ax=ax3)
        
        # 4. ç´¯ç§¯å·®å¼‚
        ax4 = axes[1, 1]
        batch_sizes = list(self.results.keys())
        cumulative_diffs = []
        
        for bs in batch_sizes:
            result = self.results[bs]
            # è®¡ç®—ç´¯ç§¯å·®å¼‚ï¼ˆæ‰€æœ‰å…ƒç´ çš„ç»å¯¹å·®å¼‚ä¹‹å’Œï¼‰
            cumulative_diff = np.sum(np.abs(result['std_output'] - result['variant_output']))
            cumulative_diffs.append(cumulative_diff)
        
        ax4.plot(batch_sizes, cumulative_diffs, 'o-', linewidth=2, markersize=8)
        ax4.set_xlabel('æ‰¹å¤„ç†å¤§å°', fontsize=12)
        ax4.set_ylabel('ç´¯ç§¯å·®å¼‚', fontsize=12)
        ax4.set_title('ç´¯ç§¯å·®å¼‚éšæ‰¹å¤„ç†å¤§å°å˜åŒ–', fontsize=14, fontweight='bold')
        ax4.set_xscale('log', base=2)
        ax4.set_yscale('log')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('experiments/plots/batch_invariant_rmsnorm_demo.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def explain_solution(self) -> None:
        """è§£é‡Šè§£å†³æ–¹æ¡ˆ"""
        print("=== Batch-invariantè§£å†³æ–¹æ¡ˆè§£é‡Š ===\n")
        
        print("ğŸ”§ é—®é¢˜æ ¹æº:")
        print("1. **å¹¶è¡Œå½’çº¦é¡ºåº**: GPUå¹¶è¡Œæ‰§è¡Œæ—¶ï¼Œå½’çº¦çš„é¡ºåºå¯èƒ½ä¸åŒ")
        print("2. **æµ®ç‚¹æ•°éç»“åˆæ€§**: (a + b) + c â‰  a + (b + c)")
        print("3. **æ‰¹å¤„ç†å¤§å°ä¾èµ–**: ä¸åŒçš„æ‰¹å¤„ç†å¤§å°å¯¼è‡´ä¸åŒçš„å¹¶è¡Œç­–ç•¥")
        print()
        
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("1. **å›ºå®šå½’çº¦é¡ºåº**: æ€»æ˜¯æŒ‰ç›¸åŒçš„é¡ºåºè¿›è¡Œå½’çº¦")
        print("2. **Batch-invariantç­–ç•¥**: ç¡®ä¿ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡º")
        print("3. **ç¡®å®šæ€§å®ç°**: é¿å…ä¾èµ–å¹¶è¡Œæ‰§è¡Œé¡ºåº")
        print()
        
        print("ğŸ“ å®ç°ç»†èŠ‚:")
        print("```python")
        print("# Batch-variant (é—®é¢˜)")
        print("for i in range(hidden_dim):")
        print("    rms_squared += x[:, :, i:i+1] ** 2  # é¡ºåºå¯èƒ½ä¸åŒ")
        print()
        print("# Batch-invariant (è§£å†³æ–¹æ¡ˆ)")
        print("for i in range(hidden_dim):")
        print("    rms_squared += x[:, :, i:i+1] ** 2  # å›ºå®šé¡ºåº")
        print("```")
        print()
        
        print("ğŸ¯ å…³é”®æ´å¯Ÿ:")
        print("â€¢ ä¸æ˜¯æ‰€æœ‰æ“ä½œéƒ½éœ€è¦batch-invariant")
        print("â€¢ åªæœ‰æ¶‰åŠå½’çº¦çš„æ“ä½œæ‰éœ€è¦")
        print("â€¢ çŸ©é˜µä¹˜æ³•æœ¬èº«æ˜¯batch-invariantçš„")
        print("â€¢ æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å½’çº¦éœ€è¦ç‰¹æ®Šå¤„ç†")
        print()
    
    def run_complete_demo(self) -> None:
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹Batch-invariant RMSNormå®Œæ•´æ¼”ç¤º...\n")
        
        # 1. æ¼”ç¤ºBatch-varianceé—®é¢˜
        self.demonstrate_batch_variance()
        
        # 2. åˆ†æBatch-invariance
        self.analyze_batch_invariance()
        
        # 3. å¯è§†åŒ–ç»“æœ
        self.visualize_batch_variance()
        
        # 4. è§£é‡Šè§£å†³æ–¹æ¡ˆ
        self.explain_solution()
        
        print("âœ… Batch-invariant RMSNormæ¼”ç¤ºå®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    demo = BatchInvariantRMSNormDemo()
    demo.run_complete_demo()

if __name__ == "__main__":
    main()
