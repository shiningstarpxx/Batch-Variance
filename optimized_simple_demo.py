#!/usr/bin/env python3
"""
ç®€åŒ–çš„ä¼˜åŒ–Batch-invariant RMSNormæ¼”ç¤º

ç»“åˆMPSå¤šæ ¸å¹¶è¡Œè®¡ç®—ï¼Œæ¼”ç¤ºçœŸæ­£çš„invariantå’Œvariantå·®å¼‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple
import time

# å¯¼å…¥å­—ä½“é…ç½®å’Œè®¾å¤‡ç®¡ç†
try:
    from src.font_config import setup_chinese_fonts
    from src.device_manager import get_device, device_manager
except ImportError:
    from font_config import setup_chinese_fonts
    from device_manager import get_device, device_manager

setup_chinese_fonts()

class OptimizedSimpleDemo:
    """ç®€åŒ–çš„ä¼˜åŒ–Batch-invariantæ¼”ç¤ºç±»"""
    
    def __init__(self, device='auto'):
        """åˆå§‹åŒ–æ¼”ç¤º"""
        if device == 'auto':
            self.device = get_device()
        else:
            self.device = get_device(device)
        
        self.device_info = device_manager.get_memory_info(self.device.type)
        self.results = {}
        
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        if self.device.type == 'mps':
            print("ğŸ ä½¿ç”¨Apple Silicon MPSå¤šæ ¸åŠ é€Ÿ")
        elif self.device.type == 'cuda':
            print("ğŸ”µ ä½¿ç”¨NVIDIA CUDAå¤šæ ¸åŠ é€Ÿ")
        else:
            print("ğŸ’» ä½¿ç”¨CPUå¤šæ ¸è®¡ç®—")
    
    def create_realistic_data(self, batch_sizes: List[int], seq_len: int = 512, 
                            hidden_dim: int = 1024) -> Dict[int, torch.Tensor]:
        """åˆ›å»ºæ›´çœŸå®çš„æµ‹è¯•æ•°æ®"""
        data = {}
        torch.manual_seed(42)
        
        for batch_size in batch_sizes:
            # åˆ›å»ºæ›´çœŸå®çš„è¾“å…¥æ•°æ®ï¼ŒåŒ…å«ä¸åŒæ•°é‡çº§çš„æ•°å€¼
            base_sample = torch.randn(seq_len, hidden_dim, device=self.device)
            
            # æ·»åŠ ä¸åŒæ•°é‡çº§çš„æ•°å€¼ï¼Œæ¨¡æ‹ŸçœŸå®æƒ…å†µ
            large_values = torch.randn(seq_len, hidden_dim // 4, device=self.device) * 10
            medium_values = torch.randn(seq_len, hidden_dim // 2, device=self.device) * 1
            small_values = torch.randn(seq_len, hidden_dim // 4, device=self.device) * 0.1
            
            # ç»„åˆä¸åŒæ•°é‡çº§çš„æ•°å€¼
            combined = torch.cat([large_values, medium_values, small_values], dim=-1)
            base_sample = base_sample + combined * 0.1
            
            batch_data = base_sample.unsqueeze(0).repeat(batch_size, 1, 1)
            data[batch_size] = batch_data
            
        return data
    
    def standard_rmsnorm(self, x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
        """æ ‡å‡†RMSNormå®ç°"""
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + eps)
        return x / rms
    
    def batch_variant_rmsnorm_chunked(self, x: torch.Tensor, chunk_size: int = 64, 
                                    eps: float = 1e-6) -> torch.Tensor:
        """Batch-variant RMSNorm - ä½¿ç”¨ä¸åŒåˆ†å—å¤§å°æ¨¡æ‹Ÿéç¡®å®šæ€§"""
        batch_size, seq_len, hidden_dim = x.shape
        
        # ä½¿ç”¨åˆ†å—å½’çº¦ï¼Œæ¨¡æ‹ŸGPUåˆ†å—å¤„ç†
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        
        for i in range(0, hidden_dim, chunk_size):
            end_idx = min(i + chunk_size, hidden_dim)
            chunk = x[:, :, i:end_idx]
            chunk_sum = torch.sum(chunk ** 2, dim=-1, keepdim=True)
            rms_squared += chunk_sum
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def batch_variant_rmsnorm_parallel_sim(self, x: torch.Tensor, num_splits: int = 4, 
                                         eps: float = 1e-6) -> torch.Tensor:
        """Batch-variant RMSNorm - æ¨¡æ‹Ÿå¹¶è¡Œå½’çº¦çš„éç¡®å®šæ€§"""
        batch_size, seq_len, hidden_dim = x.shape
        
        # æ¨¡æ‹Ÿå¹¶è¡Œå½’çº¦ï¼šå°†éšè—ç»´åº¦åˆ†æˆå¤šä¸ªéƒ¨åˆ†
        split_size = hidden_dim // num_splits
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        
        # æ¨¡æ‹Ÿä¸åŒçš„å½’çº¦é¡ºåº
        for split_idx in range(num_splits):
            start_idx = split_idx * split_size
            end_idx = start_idx + split_size if split_idx < num_splits - 1 else hidden_dim
            
            # è®¡ç®—è¿™ä¸ªåˆ†ç‰‡çš„è´¡çŒ®
            split_contribution = torch.sum(x[:, :, start_idx:end_idx] ** 2, dim=-1, keepdim=True)
            
            # æ¨¡æ‹Ÿå¹¶è¡Œå½’çº¦çš„å¾®å°å·®å¼‚
            if split_idx % 2 == 0:
                # å¶æ•°åˆ†ç‰‡ï¼šæ­£å¸¸æ·»åŠ 
                rms_squared += split_contribution
            else:
                # å¥‡æ•°åˆ†ç‰‡ï¼šæ·»åŠ å¾®å°çš„æ•°å€¼æ‰°åŠ¨
                noise = torch.randn_like(split_contribution) * 1e-10
                rms_squared += split_contribution + noise
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def batch_invariant_rmsnorm(self, x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
        """Batch-invariant RMSNorm - å›ºå®šå½’çº¦é¡ºåº"""
        batch_size, seq_len, hidden_dim = x.shape
        
        # ä½¿ç”¨å›ºå®šçš„å½’çº¦é¡ºåºï¼Œç¡®ä¿batch-invariant
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        
        # å›ºå®šé¡ºåºï¼šæ€»æ˜¯æŒ‰ç´¢å¼•é¡ºåºè¿›è¡Œå½’çº¦
        for i in range(hidden_dim):
            rms_squared += x[:, :, i:i+1] ** 2
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def batch_invariant_rmsnorm_optimized(self, x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
        """ä¼˜åŒ–çš„Batch-invariant RMSNorm - ä½¿ç”¨å›ºå®šåˆ†å—ç­–ç•¥"""
        batch_size, seq_len, hidden_dim = x.shape
        
        # ä½¿ç”¨å›ºå®šçš„åˆ†å—å¤§å°ï¼Œç¡®ä¿batch-invariant
        fixed_chunk_size = 64  # å›ºå®šåˆ†å—å¤§å°
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        
        for i in range(0, hidden_dim, fixed_chunk_size):
            end_idx = min(i + fixed_chunk_size, hidden_dim)
            chunk = x[:, :, i:end_idx]
            chunk_sum = torch.sum(chunk ** 2, dim=-1, keepdim=True)
            rms_squared += chunk_sum
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def demonstrate_optimized_variance(self) -> None:
        """æ¼”ç¤ºä¼˜åŒ–çš„varianceé—®é¢˜"""
        print("=== ä¼˜åŒ–çš„Batch-varianceæ¼”ç¤º ===\n")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_sizes = [1, 2, 4, 8]
        seq_len, hidden_dim = 256, 512
        
        print(f"ğŸ“Š æµ‹è¯•å‚æ•°:")
        print(f"   åºåˆ—é•¿åº¦: {seq_len}")
        print(f"   éšè—ç»´åº¦: {hidden_dim}")
        print(f"   æ‰¹å¤„ç†å¤§å°: {batch_sizes}")
        print(f"   è®¾å¤‡: {self.device}")
        print()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        data = self.create_realistic_data(batch_sizes, seq_len, hidden_dim)
        
        results = {}
        
        for batch_size in batch_sizes:
            print(f"ğŸ”§ æµ‹è¯•æ‰¹å¤„ç†å¤§å°: {batch_size}")
            batch_input = data[batch_size]
            
            # æ ‡å‡†RMSNorm
            std_output = self.standard_rmsnorm(batch_input)
            
            # Batch-invariant RMSNorm
            invariant_output = self.batch_invariant_rmsnorm(batch_input)
            
            # ä¼˜åŒ–çš„Batch-invariant RMSNorm
            optimized_invariant_output = self.batch_invariant_rmsnorm_optimized(batch_input)
            
            # æµ‹è¯•ä¸åŒçš„variantç­–ç•¥
            chunk_sizes = [32, 64, 128]
            num_splits = [2, 4, 8]
            
            batch_results = {
                'std_output': std_output[0].detach().cpu().numpy(),
                'invariant_output': invariant_output[0].detach().cpu().numpy(),
                'optimized_invariant_output': optimized_invariant_output[0].detach().cpu().numpy(),
                'variants': {}
            }
            
            # æµ‹è¯•ä¸åŒåˆ†å—å¤§å°
            for chunk_size in chunk_sizes:
                variant_output = self.batch_variant_rmsnorm_chunked(batch_input, chunk_size)
                diff = torch.max(torch.abs(std_output - variant_output)).item()
                
                batch_results['variants'][f'chunked_{chunk_size}'] = {
                    'output': variant_output[0].detach().cpu().numpy(),
                    'diff': diff
                }
                
                print(f"   åˆ†å—å¤§å° {chunk_size}: å·®å¼‚ {diff:.2e}")
            
            # æµ‹è¯•ä¸åŒå¹¶è¡Œåˆ†ç‰‡æ•°
            for num_split in num_splits:
                variant_output = self.batch_variant_rmsnorm_parallel_sim(batch_input, num_split)
                diff = torch.max(torch.abs(std_output - variant_output)).item()
                
                batch_results['variants'][f'parallel_{num_split}'] = {
                    'output': variant_output[0].detach().cpu().numpy(),
                    'diff': diff
                }
                
                print(f"   å¹¶è¡Œåˆ†ç‰‡ {num_split}: å·®å¼‚ {diff:.2e}")
            
            # è®¡ç®—invariantæ–¹æ³•çš„å·®å¼‚
            std_invariant_diff = torch.max(torch.abs(std_output - invariant_output)).item()
            std_optimized_diff = torch.max(torch.abs(std_output - optimized_invariant_output)).item()
            
            print(f"   Batch-invariant: å·®å¼‚ {std_invariant_diff:.2e}")
            print(f"   ä¼˜åŒ–Batch-invariant: å·®å¼‚ {std_optimized_diff:.2e}")
            print()
            
            results[batch_size] = batch_results
        
        self.results = results
        return results
    
    def analyze_optimized_effects(self) -> None:
        """åˆ†æä¼˜åŒ–æ•ˆæœ"""
        print("=== ä¼˜åŒ–æ•ˆæœåˆ†æ ===\n")
        
        if not self.results:
            print("è¯·å…ˆè¿è¡Œ demonstrate_optimized_variance()")
            return
        
        print("ğŸ” å…³é”®è§‚å¯Ÿ:")
        print("1. **åˆ†å—å¤§å°å½±å“**: ä¸åŒåˆ†å—å¤§å°äº§ç”Ÿä¸åŒç»“æœ")
        print("2. **å¹¶è¡Œåˆ†ç‰‡å½±å“**: ä¸åŒå¹¶è¡Œåˆ†ç‰‡æ•°äº§ç”Ÿä¸åŒç»“æœ")
        print("3. **Batch-invariant**: å›ºå®šç­–ç•¥ç¡®ä¿ä¸€è‡´æ€§")
        print("4. **MPSä¼˜åŒ–**: åœ¨Apple Siliconä¸Šè¡¨ç°è‰¯å¥½")
        print()
        
        # åˆ†æç¬¬ä¸€ä¸ªæ ·æœ¬åœ¨ä¸åŒç­–ç•¥ä¸‹çš„ç»“æœ
        batch_size = 4
        if batch_size in self.results:
            result = self.results[batch_size]
            print(f"ğŸ“ˆ æ‰¹å¤„ç†å¤§å° {batch_size} çš„è¯¦ç»†åˆ†æ:")
            
            std_output = result['std_output']
            invariant_output = result['invariant_output']
            
            print(f"   æ ‡å‡†RMSNormå‰5ä¸ªå€¼: {std_output[0, :5]}")
            print(f"   Batch-invariantå‰5ä¸ªå€¼: {invariant_output[0, :5]}")
            
            for variant_name, variant_result in result['variants'].items():
                variant_output = variant_result['output']
                diff = variant_result['diff']
                print(f"   {variant_name}å‰5ä¸ªå€¼: {variant_output[0, :5]} (å·®å¼‚: {diff:.2e})")
            print()
    
    def visualize_optimized_results(self) -> None:
        """å¯è§†åŒ–ä¼˜åŒ–ç»“æœ"""
        if not self.results:
            print("è¯·å…ˆè¿è¡Œ demonstrate_optimized_variance()")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ä¸åŒç­–ç•¥çš„å·®å¼‚å¯¹æ¯”
        ax1 = axes[0, 0]
        batch_sizes = list(self.results.keys())
        
        # æ”¶é›†ä¸åŒç­–ç•¥çš„å·®å¼‚
        chunk_diffs = {32: [], 64: [], 128: []}
        parallel_diffs = {2: [], 4: [], 8: []}
        
        for bs in batch_sizes:
            result = self.results[bs]
            for chunk_size in [32, 64, 128]:
                key = f'chunked_{chunk_size}'
                if key in result['variants']:
                    chunk_diffs[chunk_size].append(result['variants'][key]['diff'])
                else:
                    chunk_diffs[chunk_size].append(0)
            
            for num_split in [2, 4, 8]:
                key = f'parallel_{num_split}'
                if key in result['variants']:
                    parallel_diffs[num_split].append(result['variants'][key]['diff'])
                else:
                    parallel_diffs[num_split].append(0)
        
        # ç»˜åˆ¶åˆ†å—å¤§å°å½±å“
        for chunk_size, diffs in chunk_diffs.items():
            ax1.plot(batch_sizes, diffs, 'o-', label=f'åˆ†å—å¤§å° {chunk_size}', linewidth=2, markersize=6)
        
        ax1.set_xlabel('æ‰¹å¤„ç†å¤§å°', fontsize=12)
        ax1.set_ylabel('æœ€å¤§å·®å¼‚', fontsize=12)
        ax1.set_title('åˆ†å—å¤§å°å¯¹å·®å¼‚çš„å½±å“', fontsize=14, fontweight='bold')
        ax1.set_xscale('log', base=2)
        ax1.set_yscale('log')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å¹¶è¡Œåˆ†ç‰‡æ•°å¯¹å·®å¼‚çš„å½±å“
        ax2 = axes[0, 1]
        for num_split, diffs in parallel_diffs.items():
            ax2.plot(batch_sizes, diffs, 's-', label=f'å¹¶è¡Œåˆ†ç‰‡ {num_split}', linewidth=2, markersize=6)
        
        ax2.set_xlabel('æ‰¹å¤„ç†å¤§å°', fontsize=12)
        ax2.set_ylabel('æœ€å¤§å·®å¼‚', fontsize=12)
        ax2.set_title('å¹¶è¡Œåˆ†ç‰‡æ•°å¯¹å·®å¼‚çš„å½±å“', fontsize=14, fontweight='bold')
        ax2.set_xscale('log', base=2)
        ax2.set_yscale('log')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. è¾“å‡ºåˆ†å¸ƒå¯¹æ¯”
        ax3 = axes[1, 0]
        batch_size = 4
        if batch_size in self.results:
            result = self.results[batch_size]
            
            std_output = result['std_output'].flatten()
            invariant_output = result['invariant_output'].flatten()
            optimized_output = result['optimized_invariant_output'].flatten()
            
            ax3.hist(std_output, bins=50, alpha=0.5, label='æ ‡å‡†RMSNorm', density=True)
            ax3.hist(invariant_output, bins=50, alpha=0.5, label='Batch-invariant', density=True)
            ax3.hist(optimized_output, bins=50, alpha=0.5, label='ä¼˜åŒ–Batch-invariant', density=True)
            
            ax3.set_xlabel('è¾“å‡ºå€¼', fontsize=12)
            ax3.set_ylabel('å¯†åº¦', fontsize=12)
            ax3.set_title(f'è¾“å‡ºåˆ†å¸ƒå¯¹æ¯” (æ‰¹å¤„ç†å¤§å°={batch_size})', fontsize=14, fontweight='bold')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # 4. å·®å¼‚çƒ­å›¾
        ax4 = axes[1, 1]
        batch_size = 4
        if batch_size in self.results:
            result = self.results[batch_size]
            
            std_output = result['std_output']
            invariant_output = result['invariant_output']
            
            diff_matrix = np.abs(std_output - invariant_output)
            
            im = ax4.imshow(diff_matrix[:50, :50], cmap='Reds', aspect='auto')
            ax4.set_title('æ ‡å‡† vs Batch-invariantå·®å¼‚çƒ­å›¾', fontsize=14, fontweight='bold')
            ax4.set_xlabel('éšè—ç»´åº¦', fontsize=12)
            ax4.set_ylabel('åºåˆ—ä½ç½®', fontsize=12)
            plt.colorbar(im, ax=ax4)
        
        plt.tight_layout()
        plt.savefig('experiments/plots/optimized_simple_demo.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def benchmark_performance(self) -> None:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===\n")
        
        batch_sizes = [1, 2, 4, 8]
        seq_len, hidden_dim = 256, 512
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        data = self.create_realistic_data(batch_sizes, seq_len, hidden_dim)
        
        methods = {
            'æ ‡å‡†RMSNorm': self.standard_rmsnorm,
            'Batch-invariant': self.batch_invariant_rmsnorm,
            'ä¼˜åŒ–Batch-invariant': self.batch_invariant_rmsnorm_optimized,
            'åˆ†å—Variant (64)': lambda x: self.batch_variant_rmsnorm_chunked(x, 64),
            'å¹¶è¡ŒVariant (4)': lambda x: self.batch_variant_rmsnorm_parallel_sim(x, 4),
        }
        
        performance_results = {}
        
        for method_name, method_func in methods.items():
            print(f"ğŸ”§ æµ‹è¯•æ–¹æ³•: {method_name}")
            method_times = []
            
            for batch_size in batch_sizes:
                batch_input = data[batch_size]
                
                # é¢„çƒ­
                for _ in range(10):
                    _ = method_func(batch_input)
                
                # åŒæ­¥è®¾å¤‡
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()
                elif self.device.type == 'mps':
                    torch.mps.synchronize()
                
                # æ€§èƒ½æµ‹è¯•
                start_time = time.time()
                for _ in range(100):
                    _ = method_func(batch_input)
                
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()
                elif self.device.type == 'mps':
                    torch.mps.synchronize()
                
                end_time = time.time()
                avg_time = (end_time - start_time) / 100
                method_times.append(avg_time)
                
                print(f"   æ‰¹å¤„ç†å¤§å° {batch_size}: {avg_time*1000:.2f}ms")
            
            performance_results[method_name] = method_times
            print()
        
        # å¯è§†åŒ–æ€§èƒ½ç»“æœ
        plt.figure(figsize=(12, 8))
        
        for method_name, times in performance_results.items():
            plt.plot(batch_sizes, [t*1000 for t in times], 'o-', 
                    label=method_name, linewidth=2, markersize=8)
        
        plt.title('ä¸åŒRMSNormæ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.xlabel('æ‰¹å¤„ç†å¤§å°', fontsize=14)
        plt.ylabel('å¹³å‡æ‰§è¡Œæ—¶é—´ (ms)', fontsize=14)
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.xscale('log', base=2)
        plt.yscale('log')
        
        plt.tight_layout()
        plt.savefig('experiments/plots/optimized_performance_benchmark.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def explain_optimized_solution(self) -> None:
        """è§£é‡Šä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆ"""
        print("=== ä¼˜åŒ–çš„Batch-invariantè§£å†³æ–¹æ¡ˆè§£é‡Š ===\n")
        
        print("ğŸ”§ é—®é¢˜æ ¹æº:")
        print("1. **åˆ†å—å½’çº¦å·®å¼‚**: ä¸åŒåˆ†å—å¤§å°å¯¼è‡´ä¸åŒçš„å½’çº¦é¡ºåº")
        print("2. **å¹¶è¡Œå½’çº¦ç«äº‰**: æ¨¡æ‹ŸGPUå¹¶è¡Œæ‰§è¡Œæ—¶çš„ç«äº‰æ¡ä»¶")
        print("3. **æµ®ç‚¹æ•°éç»“åˆæ€§**: (a + b) + c â‰  a + (b + c)")
        print("4. **MPSæ¶æ„ç‰¹æ€§**: Apple Siliconçš„ç‰¹æ®Šå¹¶è¡Œå¤„ç†æ–¹å¼")
        print()
        
        print("ğŸ’¡ ä¼˜åŒ–è§£å†³æ–¹æ¡ˆ:")
        print("1. **å›ºå®šåˆ†å—ç­–ç•¥**: ä½¿ç”¨å›ºå®šçš„åˆ†å—å¤§å°ï¼Œç¡®ä¿batch-invariant")
        print("2. **ç¡®å®šæ€§å½’çº¦**: é¿å…ç«äº‰æ¡ä»¶ï¼Œç¡®ä¿å¯é‡ç°æ€§")
        print("3. **MPSä¼˜åŒ–**: é’ˆå¯¹Apple Siliconæ¶æ„ä¼˜åŒ–")
        print("4. **æ€§èƒ½å¹³è¡¡**: åœ¨ç¡®å®šæ€§å’Œæ€§èƒ½ä¹‹é—´æ‰¾åˆ°å¹³è¡¡")
        print()
        
        print("ğŸ“ å®ç°ç»†èŠ‚:")
        print("```python")
        print("# ä¼˜åŒ–çš„Batch-invariant RMSNorm")
        print("def batch_invariant_rmsnorm_optimized(x, eps=1e-6):")
        print("    fixed_chunk_size = 64  # å›ºå®šåˆ†å—å¤§å°")
        print("    rms_squared = torch.zeros_like(x[:, :, :1])")
        print("    for i in range(0, hidden_dim, fixed_chunk_size):")
        print("        chunk = x[:, :, i:i+fixed_chunk_size]")
        print("        rms_squared += torch.sum(chunk ** 2, dim=-1, keepdim=True)")
        print("    return x / torch.sqrt(rms_squared / hidden_dim + eps)")
        print("```")
        print()
        
        print("ğŸ¯ å…³é”®æ´å¯Ÿ:")
        print("â€¢ å›ºå®šåˆ†å—å¤§å°æ¯”å›ºå®šå½’çº¦é¡ºåºæ›´é«˜æ•ˆ")
        print("â€¢ åˆ†å—å¤§å°å¯¹ç»“æœæœ‰æ˜¾è‘—å½±å“")
        print("â€¢ å¹¶è¡Œåˆ†ç‰‡æ•°ä¹Ÿä¼šå½±å“ç»“æœ")
        print("â€¢ MPSåœ¨Apple Siliconä¸Šè¡¨ç°ä¼˜å¼‚")
        print("â€¢ æ€§èƒ½æŸå¤±é€šå¸¸å¾ˆå°ï¼Œä½†ç¡®å®šæ€§æ”¶ç›Šå¾ˆå¤§")
        print()
    
    def run_complete_demo(self) -> None:
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹ä¼˜åŒ–çš„Batch-invariant RMSNormå®Œæ•´æ¼”ç¤º...\n")
        
        # 1. æ¼”ç¤ºä¼˜åŒ–çš„variance
        self.demonstrate_optimized_variance()
        
        # 2. åˆ†æä¼˜åŒ–æ•ˆæœ
        self.analyze_optimized_effects()
        
        # 3. å¯è§†åŒ–ç»“æœ
        self.visualize_optimized_results()
        
        # 4. æ€§èƒ½åŸºå‡†æµ‹è¯•
        self.benchmark_performance()
        
        # 5. è§£é‡Šä¼˜åŒ–è§£å†³æ–¹æ¡ˆ
        self.explain_optimized_solution()
        
        print("âœ… ä¼˜åŒ–çš„Batch-invariant RMSNormæ¼”ç¤ºå®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    demo = OptimizedSimpleDemo(device='auto')
    demo.run_complete_demo()

if __name__ == "__main__":
    main()
