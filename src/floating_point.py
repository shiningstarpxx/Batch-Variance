"""
æµ®ç‚¹æ•°éç»“åˆæ€§æ¼”ç¤ºæ¨¡å—

è¿™ä¸ªæ¨¡å—æ¼”ç¤ºäº†æµ®ç‚¹æ•°è¿ç®—çš„éç»“åˆæ€§ç‰¹æ€§ï¼Œè¿™æ˜¯å¯¼è‡´LLMæ¨ç†éç¡®å®šæ€§çš„æ ¹æœ¬åŸå› ä¹‹ä¸€ã€‚
"""

import numpy as np
import torch
import random
from typing import List, Tuple, Dict, Any
import matplotlib.pyplot as plt
import seaborn as sns
import time

# å¯¼å…¥å­—ä½“é…ç½®å’Œè®¾å¤‡ç®¡ç†
try:
    from .font_config import setup_chinese_fonts
    from .device_manager import get_device, device_manager
except ImportError:
    from font_config import setup_chinese_fonts
    from device_manager import get_device, device_manager

# è®¾ç½®ä¸­æ–‡å­—ä½“
setup_chinese_fonts()

class FloatingPointDemo:
    """æµ®ç‚¹æ•°éç»“åˆæ€§æ¼”ç¤ºç±»"""
    
    def __init__(self, device: str = 'auto'):
        """
        åˆå§‹åŒ–æµ®ç‚¹æ•°æ¼”ç¤º
        
        Args:
            device: è®¡ç®—è®¾å¤‡ ('cpu', 'cuda', 'mps', 'auto')
        """
        if device == 'auto':
            self.device = get_device()
        else:
            self.device = get_device(device)
        
        self.results = []
        self.device_info = device_manager.get_memory_info(self.device.type)
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        if self.device.type == 'mps':
            print("ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
    
    def demonstrate_non_associativity(self) -> None:
        """æ¼”ç¤ºæµ®ç‚¹æ•°çš„éç»“åˆæ€§"""
        print("=== æµ®ç‚¹æ•°éç»“åˆæ€§æ¼”ç¤º ===")
        
        # åŸºæœ¬ç¤ºä¾‹
        a, b, c = 0.1, 1e20, -1e20
        result1 = (a + b) + c
        result2 = a + (b + c)
        
        print(f"a = {a}, b = {b}, c = {c}")
        print(f"(a + b) + c = {result1}")
        print(f"a + (b + c) = {result2}")
        print(f"ä¸¤è€…ç›¸ç­‰å—ï¼Ÿ {result1 == result2}")
        print(f"å·®å¼‚: {abs(result1 - result2)}")
        print()
    
    def demonstrate_sum_order_dependency(self, num_experiments: int = 10000) -> List[float]:
        """æ¼”ç¤ºæ±‚å’Œé¡ºåºä¾èµ–æ€§"""
        print("=== æ±‚å’Œé¡ºåºä¾èµ–æ€§æ¼”ç¤º ===")
        
        # åˆ›å»ºåŒ…å«ä¸åŒæ•°é‡çº§çš„æ•°å€¼
        vals = [1e-10, 1e-5, 1e-2, 1]
        vals = vals + [-v for v in vals]  # æ·»åŠ è´Ÿå€¼ï¼Œæ€»å’Œåº”è¯¥ä¸º0
        
        print(f"åŸå§‹æ•°ç»„: {vals}")
        print(f"ç†è®ºæ€»å’Œ: {sum(vals)}")
        print()
        
        results = []
        random.seed(42)
        
        for _ in range(num_experiments):
            # éšæœºæ‰“ä¹±æ•°ç»„é¡ºåº
            shuffled_vals = vals.copy()
            random.shuffle(shuffled_vals)
            result = sum(shuffled_vals)
            results.append(result)
        
        # ç»Ÿè®¡å”¯ä¸€ç»“æœ
        unique_results = sorted(set(results))
        print(f"ç»è¿‡ {num_experiments} æ¬¡éšæœºæ‰“ä¹±åï¼Œå¾—åˆ° {len(unique_results)} ç§ä¸åŒçš„ç»“æœ")
        print(f"ç»“æœèŒƒå›´: [{min(unique_results):.2e}, {max(unique_results):.2e}]")
        print(f"å‰10ä¸ªå”¯ä¸€ç»“æœ: {unique_results[:10]}")
        
        self.results = results
        return results
    
    def demonstrate_matrix_multiplication_determinism(self) -> None:
        """æ¼”ç¤ºçŸ©é˜µä¹˜æ³•çš„ç¡®å®šæ€§"""
        print("=== çŸ©é˜µä¹˜æ³•ç¡®å®šæ€§æ¼”ç¤º ===")
        
        # åœ¨Macä¸Šä½¿ç”¨CPUè¿›è¡Œæ¼”ç¤º
        device = 'cpu'
        dtype = torch.float32
        
        # åˆ›å»ºéšæœºçŸ©é˜µ
        A = torch.randn(512, 512, device=device, dtype=dtype)
        B = torch.randn(512, 512, device=device, dtype=dtype)
        
        # è®¡ç®—å‚è€ƒç»“æœ
        ref = torch.mm(A, B)
        
        # å¤šæ¬¡è®¡ç®—éªŒè¯ç¡®å®šæ€§
        num_tests = 100
        all_deterministic = True
        
        for i in range(num_tests):
            result = torch.mm(A, B)
            if not torch.allclose(result, ref, atol=1e-6):
                print(f"ç¬¬ {i+1} æ¬¡è®¡ç®—ä¸å‚è€ƒç»“æœä¸åŒï¼")
                all_deterministic = False
                break
        
        if all_deterministic:
            print(f"ç»è¿‡ {num_tests} æ¬¡æµ‹è¯•ï¼ŒçŸ©é˜µä¹˜æ³•ç»“æœå®Œå…¨ç¡®å®š")
            print(f"æœ€å¤§å·®å¼‚: {torch.max(torch.abs(result - ref)).item():.2e}")
        else:
            print("å‘ç°éç¡®å®šæ€§ç»“æœï¼")
    
    def demonstrate_matrix_multiplication_determinism_multi_device(self) -> None:
        """æ¼”ç¤ºCPUå’ŒMPSè®¾å¤‡ä¸ŠçŸ©é˜µä¹˜æ³•çš„ç¡®å®šæ€§"""
        print("=== å¤šè®¾å¤‡çŸ©é˜µä¹˜æ³•ç¡®å®šæ€§æ¼”ç¤º ===")
        
        # æµ‹è¯•è®¾å¤‡åˆ—è¡¨
        devices = ['cpu']
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            devices.append('mps')
        
        dtype = torch.float32
        matrix_size = 512
        num_tests = 100
        
        for device_name in devices:
            print(f"\n--- æµ‹è¯•è®¾å¤‡: {device_name.upper()} ---")
            
            device = torch.device(device_name)
            
            # åˆ›å»ºéšæœºçŸ©é˜µ
            torch.manual_seed(42)  # ç¡®ä¿æ‰€æœ‰è®¾å¤‡ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­
            A = torch.randn(matrix_size, matrix_size, device=device, dtype=dtype)
            B = torch.randn(matrix_size, matrix_size, device=device, dtype=dtype)
            
            # è®¡ç®—å‚è€ƒç»“æœ
            ref = torch.mm(A, B)
            
            # å¤šæ¬¡è®¡ç®—éªŒè¯ç¡®å®šæ€§
            all_deterministic = True
            max_diff = 0.0
            
            for i in range(num_tests):
                result = torch.mm(A, B)
                diff = torch.max(torch.abs(result - ref)).item()
                max_diff = max(max_diff, diff)
                
                if not torch.allclose(result, ref, atol=1e-6):
                    print(f"ç¬¬ {i+1} æ¬¡è®¡ç®—ä¸å‚è€ƒç»“æœä¸åŒï¼å·®å¼‚: {diff:.2e}")
                    all_deterministic = False
                    break
            
            if all_deterministic:
                print(f"âœ… ç»è¿‡ {num_tests} æ¬¡æµ‹è¯•ï¼Œ{device_name.upper()} çŸ©é˜µä¹˜æ³•ç»“æœå®Œå…¨ç¡®å®š")
                print(f"   æœ€å¤§å·®å¼‚: {max_diff:.2e}")
            else:
                print(f"âŒ åœ¨ {device_name.upper()} ä¸Šå‘ç°éç¡®å®šæ€§ç»“æœï¼")
            
            # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
            if device_name == 'mps':
                print(f"   ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
            elif device_name == 'cpu':
                print(f"   ä½¿ç”¨CPUè®¡ç®—")
        
        print(f"\n=== ç»“è®º ===")
        print("æ ‡å‡†çŸ©é˜µä¹˜æ³• (torch.mm) åœ¨CPUå’ŒMPSè®¾å¤‡ä¸Šéƒ½æ˜¯å®Œå…¨ç¡®å®šçš„")
        print("è¿™è¯æ˜äº†çŸ©é˜µä¹˜æ³•æœ¬èº«ä¸ä¼šäº§ç”Ÿéç¡®å®šæ€§ç»“æœ")
        print("\nğŸ”¬ IEEE 754æ ‡å‡†éµå¾ª:")
        print("â€¢ CUDA: NVIDIAå®˜æ–¹æ˜ç¡®å£°æ˜å®Œå…¨éµå¾ªIEEE 754æ ‡å‡†")
        print("â€¢ MPS: åŸºäºPyTorchå®ç°å’Œæµ‹è¯•ç»“æœæ¨æµ‹éµå¾ªIEEE 754æ ‡å‡†")
        print("â€¢ æ‰€æœ‰è®¾å¤‡éƒ½ç¡®ä¿æµ®ç‚¹è¿ç®—çš„ç¡®å®šæ€§å’Œä¸€è‡´æ€§")
        print("â€¢ è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆåŸºç¡€æ•°å­¦è¿ç®—ä¸ä¼šäº§ç”Ÿéç¡®å®šæ€§ç»“æœ")
    
    def visualize_sum_distribution(self, save_path: str = None) -> None:
        """å¯è§†åŒ–æ±‚å’Œç»“æœåˆ†å¸ƒ"""
        if not self.results:
            print("è¯·å…ˆè¿è¡Œ demonstrate_sum_order_dependency()")
            return
        
        plt.figure(figsize=(12, 8))
        
        # å­å›¾1: ç›´æ–¹å›¾
        plt.subplot(2, 2, 1)
        plt.hist(self.results, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        plt.title('æ±‚å’Œç»“æœåˆ†å¸ƒç›´æ–¹å›¾', fontsize=14)
        plt.xlabel('æ±‚å’Œç»“æœ', fontsize=12)
        plt.ylabel('é¢‘æ¬¡', fontsize=12)
        plt.grid(True, alpha=0.3)
        
        # å­å›¾2: ç®±çº¿å›¾
        plt.subplot(2, 2, 2)
        plt.boxplot(self.results, vert=True)
        plt.title('æ±‚å’Œç»“æœç®±çº¿å›¾', fontsize=14)
        plt.ylabel('æ±‚å’Œç»“æœ', fontsize=12)
        plt.grid(True, alpha=0.3)
        
        # å­å›¾3: æ—¶é—´åºåˆ—
        plt.subplot(2, 2, 3)
        plt.plot(self.results[:1000], alpha=0.7, color='green')
        plt.title('å‰1000æ¬¡å®éªŒç»“æœ', fontsize=14)
        plt.xlabel('å®éªŒæ¬¡æ•°', fontsize=12)
        plt.ylabel('æ±‚å’Œç»“æœ', fontsize=12)
        plt.grid(True, alpha=0.3)
        
        # å­å›¾4: ç´¯ç§¯åˆ†å¸ƒ
        plt.subplot(2, 2, 4)
        sorted_results = sorted(self.results)
        y = np.arange(1, len(sorted_results) + 1) / len(sorted_results)
        plt.plot(sorted_results, y, color='red', linewidth=2)
        plt.title('ç´¯ç§¯åˆ†å¸ƒå‡½æ•°', fontsize=14)
        plt.xlabel('æ±‚å’Œç»“æœ', fontsize=12)
        plt.ylabel('ç´¯ç§¯æ¦‚ç‡', fontsize=12)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
    
    def run_complete_demo(self) -> None:
        """è¿è¡Œå®Œæ•´çš„æµ®ç‚¹æ•°éç»“åˆæ€§æ¼”ç¤º"""
        print("å¼€å§‹æµ®ç‚¹æ•°éç»“åˆæ€§å®Œæ•´æ¼”ç¤º...\n")
        
        # 1. åŸºæœ¬éç»“åˆæ€§æ¼”ç¤º
        self.demonstrate_non_associativity()
        
        # 2. æ±‚å’Œé¡ºåºä¾èµ–æ€§æ¼”ç¤º
        self.demonstrate_sum_order_dependency()
        
        # 3. çŸ©é˜µä¹˜æ³•ç¡®å®šæ€§æ¼”ç¤ºï¼ˆå•è®¾å¤‡ï¼‰
        self.demonstrate_matrix_multiplication_determinism()
        
        # 4. çŸ©é˜µä¹˜æ³•ç¡®å®šæ€§æ¼”ç¤ºï¼ˆå¤šè®¾å¤‡ï¼‰
        self.demonstrate_matrix_multiplication_determinism_multi_device()
        
        # 5. å¯è§†åŒ–ç»“æœ
        self.visualize_sum_distribution('experiments/plots/floating_point_demo.png')
        
        print("\næµ®ç‚¹æ•°éç»“åˆæ€§æ¼”ç¤ºå®Œæˆï¼")
    
    def multi_dimensional_analysis(self, dimensions: List[int] = [64, 128, 256, 512, 1024]) -> Dict[str, Any]:
        """å¤šç»´åº¦åˆ†ææµ®ç‚¹æ•°éç»“åˆæ€§"""
        print("=== å¤šç»´åº¦æµ®ç‚¹æ•°éç»“åˆæ€§åˆ†æ ===")
        
        results = {}
        
        for dim in dimensions:
            print(f"æµ‹è¯•ç»´åº¦: {dim}x{dim}")
            
            # åˆ›å»ºæµ‹è¯•çŸ©é˜µ
            a = torch.randn(dim, dim, device=self.device, dtype=torch.float32)
            b = torch.randn(dim, dim, device=self.device, dtype=torch.float32)
            c = torch.randn(dim, dim, device=self.device, dtype=torch.float32)
            
            # æµ‹è¯•ä¸åŒçš„è®¡ç®—é¡ºåº
            start_time = time.time()
            
            # é¡ºåº1: (A + B) + C
            result1 = (a + b) + c
            
            # é¡ºåº2: A + (B + C)
            result2 = a + (b + c)
            
            # åŒæ­¥è®¾å¤‡
            if self.device.type == 'cuda':
                torch.cuda.synchronize()
            elif self.device.type == 'mps':
                torch.mps.synchronize()
            
            end_time = time.time()
            
            # è®¡ç®—å·®å¼‚
            difference = torch.abs(result1 - result2)
            max_diff = torch.max(difference).item()
            mean_diff = torch.mean(difference).item()
            std_diff = torch.std(difference).item()
            
            results[dim] = {
                'max_difference': max_diff,
                'mean_difference': mean_diff,
                'std_difference': std_diff,
                'computation_time': end_time - start_time,
                'matrix_size': dim * dim
            }
            
            print(f"  æœ€å¤§å·®å¼‚: {max_diff:.2e}")
            print(f"  å¹³å‡å·®å¼‚: {mean_diff:.2e}")
            print(f"  è®¡ç®—æ—¶é—´: {(end_time - start_time)*1000:.2f}ms")
        
        return results
    
    def precision_analysis(self, precision_levels: List[str] = ['float32', 'float64']) -> Dict[str, Any]:
        """ç²¾åº¦åˆ†æ"""
        print("=== ç²¾åº¦åˆ†æ ===")
        
        results = {}
        dim = 256
        
        for precision in precision_levels:
            print(f"æµ‹è¯•ç²¾åº¦: {precision}")
            
            # MPSä¸æ”¯æŒfloat64ï¼Œéœ€è¦å›é€€åˆ°CPU
            if precision == 'float64' and self.device.type == 'mps':
                print("  MPSä¸æ”¯æŒfloat64ï¼Œä½¿ç”¨CPUè¿›è¡Œæµ‹è¯•")
                device = torch.device('cpu')
            else:
                device = self.device
            
            dtype = torch.float32 if precision == 'float32' else torch.float64
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            a = torch.randn(dim, dim, device=device, dtype=dtype)
            b = torch.randn(dim, dim, device=device, dtype=dtype)
            c = torch.randn(dim, dim, device=device, dtype=dtype)
            
            # æµ‹è¯•éç»“åˆæ€§
            result1 = (a + b) + c
            result2 = a + (b + c)
            
            difference = torch.abs(result1 - result2)
            max_diff = torch.max(difference).item()
            mean_diff = torch.mean(difference).item()
            
            results[precision] = {
                'max_difference': max_diff,
                'mean_difference': mean_diff,
                'dtype': dtype
            }
            
            print(f"  æœ€å¤§å·®å¼‚: {max_diff:.2e}")
            print(f"  å¹³å‡å·®å¼‚: {mean_diff:.2e}")
        
        return results
    
    def device_comparison(self) -> Dict[str, Any]:
        """è®¾å¤‡æ€§èƒ½å¯¹æ¯”"""
        print("=== è®¾å¤‡æ€§èƒ½å¯¹æ¯” ===")
        
        # è·å–æ‰€æœ‰å¯ç”¨è®¾å¤‡
        available_devices = ['cpu']
        if torch.cuda.is_available():
            available_devices.append('cuda')
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            available_devices.append('mps')
        
        results = {}
        dim = 512
        
        for device_name in available_devices:
            print(f"æµ‹è¯•è®¾å¤‡: {device_name}")
            
            device = get_device(device_name)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            a = torch.randn(dim, dim, device=device, dtype=torch.float32)
            b = torch.randn(dim, dim, device=device, dtype=torch.float32)
            c = torch.randn(dim, dim, device=device, dtype=torch.float32)
            
            # é¢„çƒ­
            for _ in range(5):
                _ = (a + b) + c
            
            # åŒæ­¥è®¾å¤‡
            if device.type == 'cuda':
                torch.cuda.synchronize()
            elif device.type == 'mps':
                torch.mps.synchronize()
            
            # æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            for _ in range(10):
                result1 = (a + b) + c
                result2 = a + (b + c)
            end_time = time.time()
            
            # åŒæ­¥è®¾å¤‡
            if device.type == 'cuda':
                torch.cuda.synchronize()
            elif device.type == 'mps':
                torch.mps.synchronize()
            
            avg_time = (end_time - start_time) / 10
            
            # è®¡ç®—å·®å¼‚
            difference = torch.abs(result1 - result2)
            max_diff = torch.max(difference).item()
            
            results[device_name] = {
                'avg_time_ms': avg_time * 1000,
                'max_difference': max_diff,
                'device': device_name
            }
            
            print(f"  å¹³å‡æ—¶é—´: {avg_time*1000:.2f}ms")
            print(f"  æœ€å¤§å·®å¼‚: {max_diff:.2e}")
        
        return results

def create_floating_point_examples() -> List[Tuple[str, float, float]]:
    """åˆ›å»ºæ›´å¤šæµ®ç‚¹æ•°éç»“åˆæ€§ç¤ºä¾‹"""
    examples = []
    
    # ç¤ºä¾‹1: ä¸åŒæ•°é‡çº§
    a, b, c = 0.1, 1e20, -1e20
    result1 = (a + b) + c
    result2 = a + (b + c)
    examples.append(("ä¸åŒæ•°é‡çº§", result1, result2))
    
    # ç¤ºä¾‹2: ç²¾åº¦æŸå¤±
    a, b, c = 1e-10, 1e-5, 1e-2
    result1 = (a + b) + c
    result2 = a + (b + c)
    examples.append(("ç²¾åº¦æŸå¤±", result1, result2))
    
    # ç¤ºä¾‹3: å¤§æ•°ç›¸åŠ 
    a, b, c = 1e15, 1e-10, -1e15
    result1 = (a + b) + c
    result2 = a + (b + c)
    examples.append(("å¤§æ•°ç›¸åŠ ", result1, result2))
    
    return examples

if __name__ == "__main__":
    demo = FloatingPointDemo()
    demo.run_complete_demo()
