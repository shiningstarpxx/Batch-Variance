#!/usr/bin/env python3
"""
MPSå¹¶è¡ŒRMSNormæ¼”ç¤º

å±•ç¤ºåœ¨åˆ†å—åŸºç¡€ä¸Šå¢åŠ MPSå¹¶è¡Œè®¡ç®—ï¼š
- æ€§èƒ½æå‡ï¼šåˆ©ç”¨MPSå¤šæ ¸å¹¶è¡Œ
- å¼•å…¥variantï¼šå¹¶è¡Œæ‰§è¡Œå¯¼è‡´éç¡®å®šæ€§
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from typing import Dict, List, Tuple
import time
import concurrent.futures
import threading

# å¯¼å…¥å­—ä½“é…ç½®
try:
    from src.font_config import setup_chinese_fonts
except ImportError:
    from font_config import setup_chinese_fonts

setup_chinese_fonts()

class MPSParallelRMSNormDemo:
    """MPSå¹¶è¡ŒRMSNormæ¼”ç¤ºå™¨"""
    
    def __init__(self, device='cpu'):
        """åˆå§‹åŒ–æ¼”ç¤ºå™¨"""
        self.device = torch.device(device)
        self.results = {}
        
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        if self.device.type == 'mps':
            print("   âœ… MPSå¯ç”¨ï¼Œå°†ä½¿ç”¨å¹¶è¡Œè®¡ç®—")
        else:
            print("   âš ï¸ MPSä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡æ‹Ÿ")
    
    def create_test_data(self, batch_size: int = 4, seq_len: int = 256, 
                        hidden_dim: int = 512) -> torch.Tensor:
        """åˆ›å»ºæµ‹è¯•æ•°æ®"""
        torch.manual_seed(42)
        
        # åˆ›å»ºåŒ…å«ä¸åŒæ•°é‡çº§çš„çœŸå®æ•°æ®
        base_sample = torch.randn(seq_len, hidden_dim, device=self.device)
        
        # æ·»åŠ ä¸åŒæ•°é‡çº§çš„æ•°å€¼
        large_values = torch.randn(seq_len, hidden_dim // 4, device=self.device) * 10
        medium_values = torch.randn(seq_len, hidden_dim // 2, device=self.device) * 1
        small_values = torch.randn(seq_len, hidden_dim // 4, device=self.device) * 0.1
        
        combined = torch.cat([large_values, medium_values, small_values], dim=-1)
        base_sample = base_sample + combined * 0.1
        
        batch_data = base_sample.unsqueeze(0).repeat(batch_size, 1, 1)
        return batch_data
    
    def standard_rmsnorm(self, x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
        """æ ‡å‡†RMSNormå®ç°ï¼ˆç¡®å®šæ€§ï¼‰"""
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + eps)
        return x / rms
    
    def chunked_rmsnorm_sequential(self, x: torch.Tensor, chunk_size: int = 64, 
                                  eps: float = 1e-6) -> torch.Tensor:
        """åˆ†å—RMSNorm - é¡ºåºæ‰§è¡Œï¼ˆç¡®å®šæ€§ï¼‰"""
        batch_size, seq_len, hidden_dim = x.shape
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        
        for i in range(0, hidden_dim, chunk_size):
            end_idx = min(i + chunk_size, hidden_dim)
            chunk = x[:, :, i:end_idx]
            chunk_sum = torch.sum(chunk ** 2, dim=-1, keepdim=True)
            rms_squared += chunk_sum
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def chunked_rmsnorm_parallel_cpu(self, x: torch.Tensor, chunk_size: int = 64, 
                                    num_threads: int = 4, eps: float = 1e-6) -> torch.Tensor:
        """åˆ†å—RMSNorm - CPUå¹¶è¡Œæ‰§è¡Œï¼ˆéç¡®å®šæ€§ï¼‰"""
        batch_size, seq_len, hidden_dim = x.shape
        
        # è®¡ç®—åˆ†å—æ•°é‡
        num_chunks = (hidden_dim + chunk_size - 1) // chunk_size
        
        # åˆ›å»ºåˆ†å—ç´¢å¼•
        chunk_indices = []
        for i in range(0, hidden_dim, chunk_size):
            end_idx = min(i + chunk_size, hidden_dim)
            chunk_indices.append((i, end_idx))
        
        # å¹¶è¡Œè®¡ç®—æ¯ä¸ªåˆ†å—
        chunk_results = [None] * len(chunk_indices)
        
        def compute_chunk(chunk_idx, start_idx, end_idx):
            chunk = x[:, :, start_idx:end_idx]
            chunk_sum = torch.sum(chunk ** 2, dim=-1, keepdim=True)
            chunk_results[chunk_idx] = chunk_sum
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = []
            for i, (start_idx, end_idx) in enumerate(chunk_indices):
                future = executor.submit(compute_chunk, i, start_idx, end_idx)
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            concurrent.futures.wait(futures)
        
        # åˆå¹¶ç»“æœï¼ˆéç¡®å®šæ€§é¡ºåºï¼‰
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        for chunk_sum in chunk_results:
            rms_squared += chunk_sum
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def chunked_rmsnorm_parallel_mps(self, x: torch.Tensor, chunk_size: int = 64, 
                                    num_streams: int = 4, eps: float = 1e-6) -> torch.Tensor:
        """åˆ†å—RMSNorm - MPSå¹¶è¡Œæ‰§è¡Œï¼ˆéç¡®å®šæ€§ï¼‰"""
        if self.device.type != 'mps':
            # å¦‚æœMPSä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUå¹¶è¡Œ
            return self.chunked_rmsnorm_parallel_cpu(x, chunk_size, num_streams, eps)
        
        batch_size, seq_len, hidden_dim = x.shape
        
        # è®¡ç®—åˆ†å—æ•°é‡
        num_chunks = (hidden_dim + chunk_size - 1) // chunk_size
        
        # åˆ›å»ºåˆ†å—ç´¢å¼•
        chunk_indices = []
        for i in range(0, hidden_dim, chunk_size):
            end_idx = min(i + chunk_size, hidden_dim)
            chunk_indices.append((i, end_idx))
        
        # æ¨¡æ‹ŸMPSå¹¶è¡Œè®¡ç®—ï¼ˆå®é™…ä¸­MPSä¼šè‡ªåŠ¨å¹¶è¡ŒåŒ–ï¼‰
        chunk_results = []
        
        # MPSæ²¡æœ‰CUDAæµï¼Œä½¿ç”¨çº¿ç¨‹æ¨¡æ‹Ÿå¹¶è¡Œ
        streams = [None] * num_streams
        
        def compute_chunk_mps(chunk_idx, start_idx, end_idx, stream_idx):
            if streams[stream_idx] is not None:
                with torch.cuda.stream(streams[stream_idx]):
                    chunk = x[:, :, start_idx:end_idx]
                    chunk_sum = torch.sum(chunk ** 2, dim=-1, keepdim=True)
                    chunk_results.append(chunk_sum)
            else:
                # MPSå›é€€åˆ°çº¿ç¨‹
                chunk = x[:, :, start_idx:end_idx]
                chunk_sum = torch.sum(chunk ** 2, dim=-1, keepdim=True)
                chunk_results.append(chunk_sum)
        
        # å¹¶è¡Œè®¡ç®—åˆ†å—
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_streams) as executor:
            futures = []
            for i, (start_idx, end_idx) in enumerate(chunk_indices):
                stream_idx = i % num_streams
                future = executor.submit(compute_chunk_mps, i, start_idx, end_idx, stream_idx)
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            concurrent.futures.wait(futures)
        
        # åˆå¹¶ç»“æœï¼ˆéç¡®å®šæ€§é¡ºåºï¼‰
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        for chunk_sum in chunk_results:
            rms_squared += chunk_sum
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def batch_invariant_rmsnorm_mps(self, x: torch.Tensor, chunk_size: int = 64, 
                                   eps: float = 1e-6) -> torch.Tensor:
        """Batch-invariant RMSNorm - MPSä¼˜åŒ–ï¼ˆç¡®å®šæ€§ï¼‰"""
        # ä½¿ç”¨ä¸æ ‡å‡†RMSNormç›¸åŒçš„ç®—æ³•ï¼Œä½†åˆ©ç”¨MPSåŠ é€Ÿ
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + eps)
        return x / rms
    
    def test_performance_and_variance(self) -> Dict:
        """æµ‹è¯•æ€§èƒ½å’Œæ–¹å·®"""
        print("=== MPSå¹¶è¡ŒRMSNormæ€§èƒ½ä¸æ–¹å·®æµ‹è¯• ===\n")
        
        # æµ‹è¯•å‚æ•°
        batch_size, seq_len, hidden_dim = 4, 256, 512
        num_tests = 10
        
        print(f"ğŸ“Š æµ‹è¯•å‚æ•°:")
        print(f"   æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"   åºåˆ—é•¿åº¦: {seq_len}")
        print(f"   éšè—ç»´åº¦: {hidden_dim}")
        print(f"   æµ‹è¯•æ¬¡æ•°: {num_tests}")
        print()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = self.create_test_data(batch_size, seq_len, hidden_dim)
        
        # å®šä¹‰æµ‹è¯•æ–¹æ³•
        methods = {
            'æ ‡å‡†RMSNorm': self.standard_rmsnorm,
            'åˆ†å—é¡ºåº(64)': lambda x: self.chunked_rmsnorm_sequential(x, 64),
            'åˆ†å—å¹¶è¡ŒCPU(4çº¿ç¨‹)': lambda x: self.chunked_rmsnorm_parallel_cpu(x, 64, 4),
            'åˆ†å—å¹¶è¡ŒMPS(4æµ)': lambda x: self.chunked_rmsnorm_parallel_mps(x, 64, 4),
            'Batch-invariant MPS': self.batch_invariant_rmsnorm_mps,
        }
        
        results = {}
        
        for method_name, method_func in methods.items():
            print(f"ğŸ”§ æµ‹è¯•æ–¹æ³•: {method_name}")
            
            # æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            for _ in range(num_tests):
                result = method_func(test_data)
            end_time = time.time()
            avg_time = (end_time - start_time) / num_tests * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æ–¹å·®æµ‹è¯•
            outputs = []
            for _ in range(num_tests):
                output = method_func(test_data)
                outputs.append(output)
            
            # è®¡ç®—æ–¹å·®
            reference = outputs[0]
            max_diffs = []
            for output in outputs[1:]:
                diff = torch.max(torch.abs(output - reference)).item()
                max_diffs.append(diff)
            
            avg_diff = np.mean(max_diffs) if max_diffs else 0.0
            max_diff = np.max(max_diffs) if max_diffs else 0.0
            
            results[method_name] = {
                'avg_time_ms': avg_time,
                'avg_diff': avg_diff,
                'max_diff': max_diff,
                'is_deterministic': max_diff < 1e-10
            }
            
            print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f} ms")
            print(f"   å¹³å‡å·®å¼‚: {avg_diff:.2e}")
            print(f"   æœ€å¤§å·®å¼‚: {max_diff:.2e}")
            print(f"   ç¡®å®šæ€§: {'âœ… æ˜¯' if max_diff < 1e-10 else 'âŒ å¦'}")
            print()
        
        self.results = results
        return results
    
    def create_performance_comparison(self) -> None:
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾"""
        if not self.results:
            print("è¯·å…ˆè¿è¡Œ test_performance_and_variance()")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æ€§èƒ½å¯¹æ¯”
        ax1 = axes[0, 0]
        methods = list(self.results.keys())
        times = [self.results[method]['avg_time_ms'] for method in methods]
        colors = ['green' if self.results[method]['is_deterministic'] else 'red' for method in methods]
        
        bars = ax1.bar(methods, times, color=colors, alpha=0.7)
        ax1.set_ylabel('å¹³å‡æ‰§è¡Œæ—¶é—´ (ms)', fontsize=12)
        ax1.set_title('MPSå¹¶è¡ŒRMSNormæ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for bar, time in zip(bars, times):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{time:.1f}ms', ha='center', va='bottom', fontsize=10)
        
        # 2. æ–¹å·®å¯¹æ¯”
        ax2 = axes[0, 1]
        max_diffs = [self.results[method]['max_diff'] for method in methods]
        
        bars = ax2.bar(methods, max_diffs, color=colors, alpha=0.7)
        ax2.set_ylabel('æœ€å¤§å·®å¼‚', fontsize=12)
        ax2.set_title('MPSå¹¶è¡ŒRMSNormæ–¹å·®å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_yscale('log')
        ax2.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for bar, diff in zip(bars, max_diffs):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height * 1.1,
                    f'{diff:.1e}', ha='center', va='bottom', fontsize=10)
        
        # 3. æ€§èƒ½vsæ–¹å·®æ•£ç‚¹å›¾
        ax3 = axes[1, 0]
        for i, method in enumerate(methods):
            time = self.results[method]['avg_time_ms']
            diff = self.results[method]['max_diff']
            color = 'green' if self.results[method]['is_deterministic'] else 'red'
            ax3.scatter(time, diff, color=color, s=100, alpha=0.7, label=method)
        
        ax3.set_xlabel('å¹³å‡æ‰§è¡Œæ—¶é—´ (ms)', fontsize=12)
        ax3.set_ylabel('æœ€å¤§å·®å¼‚', fontsize=12)
        ax3.set_title('æ€§èƒ½ vs æ–¹å·®', fontsize=14, fontweight='bold')
        ax3.set_yscale('log')
        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax3.grid(True, alpha=0.3)
        
        # 4. ç¡®å®šæ€§ç»Ÿè®¡
        ax4 = axes[1, 1]
        deterministic_count = sum(1 for result in self.results.values() if result['is_deterministic'])
        non_deterministic_count = len(self.results) - deterministic_count
        
        labels = ['ç¡®å®šæ€§', 'éç¡®å®šæ€§']
        sizes = [deterministic_count, non_deterministic_count]
        colors = ['green', 'red']
        
        wedges, texts, autotexts = ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        ax4.set_title('ç¡®å®šæ€§ç»Ÿè®¡', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('experiments/plots/mps_parallel_rmsnorm_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def print_summary(self) -> None:
        """æ‰“å°æ€»ç»“"""
        if not self.results:
            print("è¯·å…ˆè¿è¡Œ test_performance_and_variance()")
            return
        
        print("=== MPSå¹¶è¡ŒRMSNormæ€»ç»“ ===\n")
        
        # åˆ›å»ºæ±‡æ€»è¡¨
        summary_data = []
        for method, result in self.results.items():
            summary_data.append({
                'æ–¹æ³•': method,
                'å¹³å‡æ—¶é—´(ms)': f"{result['avg_time_ms']:.2f}",
                'æœ€å¤§å·®å¼‚': f"{result['max_diff']:.2e}",
                'ç¡®å®šæ€§': 'âœ… æ˜¯' if result['is_deterministic'] else 'âŒ å¦',
                'æ€§èƒ½ç­‰çº§': self._get_performance_level(result['avg_time_ms']),
                'æ¨èåº¦': self._get_recommendation(method, result)
            })
        
        df = pd.DataFrame(summary_data)
        print("ğŸ“Š æ±‡æ€»è¡¨:")
        print(df.to_string(index=False))
        print()
        
        # å…³é”®å‘ç°
        print("ğŸ¯ å…³é”®å‘ç°:")
        print("1. **æ€§èƒ½æå‡**: MPSå¹¶è¡Œè®¡ç®—æ˜¾è‘—æå‡æ€§èƒ½")
        print("2. **å¼•å…¥æ–¹å·®**: å¹¶è¡Œæ‰§è¡Œå¯¼è‡´éç¡®å®šæ€§ç»“æœ")
        print("3. **Batch-invariant**: ä½¿ç”¨æ ‡å‡†ç®—æ³•ä¿æŒç¡®å®šæ€§")
        print("4. **æƒè¡¡**: æ€§èƒ½ vs ç¡®å®šæ€§çš„æƒè¡¡")
        print()
        
        # æ¨è
        print("ğŸ’¡ æ¨è:")
        print("â€¢ **ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨Batch-invariant MPSï¼ˆç¡®å®šæ€§+é«˜æ€§èƒ½ï¼‰")
        print("â€¢ **ç ”ç©¶ç¯å¢ƒ**: å¯ä»¥ä½¿ç”¨å¹¶è¡ŒMPSï¼ˆé«˜æ€§èƒ½ä½†éç¡®å®šæ€§ï¼‰")
        print("â€¢ **é¿å…**: çº¯CPUå¹¶è¡Œï¼ˆæ€§èƒ½å·®ä¸”éç¡®å®šæ€§ï¼‰")
    
    def _get_performance_level(self, time_ms: float) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        if time_ms < 1.0:
            return "ä¼˜ç§€"
        elif time_ms < 5.0:
            return "è‰¯å¥½"
        elif time_ms < 20.0:
            return "ä¸€èˆ¬"
        else:
            return "è¾ƒå·®"
    
    def _get_recommendation(self, method: str, result: Dict) -> str:
        """è·å–æ¨èåº¦"""
        if method == 'Batch-invariant MPS':
            return "âœ… å¼ºçƒˆæ¨è"
        elif method == 'åˆ†å—å¹¶è¡ŒMPS(4æµ)':
            return "âš ï¸ å¯é€‰"
        elif method == 'æ ‡å‡†RMSNorm':
            return "âœ… æ¨è"
        elif method == 'åˆ†å—é¡ºåº(64)':
            return "âš ï¸ å¯é€‰"
        else:
            return "âŒ ä¸æ¨è"
    
    def run_complete_demo(self) -> None:
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹MPSå¹¶è¡ŒRMSNormæ¼”ç¤º...\n")
        
        # 1. æµ‹è¯•æ€§èƒ½å’Œæ–¹å·®
        self.test_performance_and_variance()
        
        # 2. åˆ›å»ºå¯¹æ¯”å›¾
        self.create_performance_comparison()
        
        # 3. æ‰“å°æ€»ç»“
        self.print_summary()
        
        print("âœ… MPSå¹¶è¡ŒRMSNormæ¼”ç¤ºå®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    # å°è¯•ä½¿ç”¨MPSï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨CPU
    device = 'mps' if torch.backends.mps.is_available() else 'cpu'
    demo = MPSParallelRMSNormDemo(device=device)
    demo.run_complete_demo()

if __name__ == "__main__":
    main()
