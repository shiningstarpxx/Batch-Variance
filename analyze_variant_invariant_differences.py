#!/usr/bin/env python3
"""
åˆ†æä¸ºä»€ä¹ˆVariantæ²¡æœ‰å·®å¼‚è€ŒInvariantæœ‰å·®å¼‚

æ·±å…¥åˆ†æå¹¶è¡Œè®¡ç®—çš„ç¡®å®šæ€§è¡Œä¸º
"""

import torch
import numpy as np
import time
import concurrent.futures
import threading
from typing import List, Dict

class VariantInvariantAnalyzer:
    """Variant vs Invariantå·®å¼‚åˆ†æå™¨"""
    
    def __init__(self, device='cpu'):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.device = torch.device(device)
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def create_test_data(self, batch_size: int = 2, seq_len: int = 4, 
                        hidden_dim: int = 8) -> torch.Tensor:
        """åˆ›å»ºå°è§„æ¨¡æµ‹è¯•æ•°æ®ä¾¿äºåˆ†æ"""
        torch.manual_seed(42)
        test_data = torch.randn(batch_size, seq_len, hidden_dim, device=self.device)
        return test_data
    
    def variant_implementation(self, x: torch.Tensor, chunk_size: int = 2, 
                             num_threads: int = 2, eps: float = 1e-6) -> torch.Tensor:
        """Variantå®ç°ï¼šéç¡®å®šæ€§åˆå¹¶"""
        batch_size, seq_len, hidden_dim = x.shape
        
        # åˆ›å»ºåˆ†å—ç´¢å¼•
        chunk_indices = []
        for i in range(0, hidden_dim, chunk_size):
            end_idx = min(i + chunk_size, hidden_dim)
            chunk_indices.append((i, end_idx))
        
        # å¹¶è¡Œè®¡ç®—æ¯ä¸ªåˆ†å—
        chunk_results = [None] * len(chunk_indices)
        
        def compute_chunk(chunk_idx, start_idx, end_idx):
            chunk = x[:, :, start_idx:end_idx]
            chunk_sum = torch.sum(chunk ** 2, dim=-1, keepdim=True)
            chunk_results[chunk_idx] = chunk_sum
            print(f"    çº¿ç¨‹ {threading.current_thread().ident}: è®¡ç®—åˆ†å— {chunk_idx}, ç»“æœ: {chunk_sum[0, 0, 0].item():.6f}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = []
            for i, (start_idx, end_idx) in enumerate(chunk_indices):
                future = executor.submit(compute_chunk, i, start_idx, end_idx)
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            concurrent.futures.wait(futures)
        
        print(f"    åˆ†å—ç»“æœé¡ºåº: {[chunk_results[i][0, 0, 0].item() for i in range(len(chunk_results))]}")
        
        # åˆå¹¶ç»“æœï¼ˆéç¡®å®šæ€§é¡ºåºï¼‰
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        for chunk_sum in chunk_results:
            rms_squared += chunk_sum
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def invariant_implementation(self, x: torch.Tensor, chunk_size: int = 2, 
                               num_threads: int = 2, eps: float = 1e-6) -> torch.Tensor:
        """Invariantå®ç°ï¼šç¡®å®šæ€§åˆå¹¶"""
        batch_size, seq_len, hidden_dim = x.shape
        
        # åˆ›å»ºåˆ†å—ç´¢å¼•
        chunk_indices = []
        for i in range(0, hidden_dim, chunk_size):
            end_idx = min(i + chunk_size, hidden_dim)
            chunk_indices.append((i, end_idx))
        
        # å¹¶è¡Œè®¡ç®—æ¯ä¸ªåˆ†å—
        chunk_results = [None] * len(chunk_indices)
        
        def compute_chunk(chunk_idx, start_idx, end_idx):
            chunk = x[:, :, start_idx:end_idx]
            chunk_sum = torch.sum(chunk ** 2, dim=-1, keepdim=True)
            chunk_results[chunk_idx] = chunk_sum
            print(f"    çº¿ç¨‹ {threading.current_thread().ident}: è®¡ç®—åˆ†å— {chunk_idx}, ç»“æœ: {chunk_sum[0, 0, 0].item():.6f}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = []
            for i, (start_idx, end_idx) in enumerate(chunk_indices):
                future = executor.submit(compute_chunk, i, start_idx, end_idx)
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            concurrent.futures.wait(futures)
        
        print(f"    åˆ†å—ç»“æœé¡ºåº: {[chunk_results[i][0, 0, 0].item() for i in range(len(chunk_results))]}")
        
        # åˆå¹¶ç»“æœï¼ˆç¡®å®šæ€§é¡ºåºï¼šæŒ‰ç´¢å¼•é¡ºåºï¼‰
        rms_squared = torch.zeros(batch_size, seq_len, 1, device=x.device)
        for i in range(len(chunk_results)):
            if chunk_results[i] is not None:
                rms_squared += chunk_results[i]
                print(f"    æŒ‰ç´¢å¼• {i} åˆå¹¶: {chunk_results[i][0, 0, 0].item():.6f}")
        
        rms = torch.sqrt(rms_squared / hidden_dim + eps)
        return x / rms
    
    def analyze_determinism(self) -> None:
        """åˆ†æç¡®å®šæ€§è¡Œä¸º"""
        print("=== åˆ†æVariant vs Invariantçš„ç¡®å®šæ€§è¡Œä¸º ===\n")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = self.create_test_data()
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
        print(f"   æ•°æ®: {test_data[0, 0, :].numpy()}")
        print()
        
        # æµ‹è¯•æ¬¡æ•°
        num_tests = 5
        
        print("ğŸ”§ æµ‹è¯•Variantå®ç°ï¼ˆéç¡®å®šæ€§åˆå¹¶ï¼‰:")
        variant_outputs = []
        for i in range(num_tests):
            print(f"  ç¬¬{i+1}æ¬¡æ‰§è¡Œ:")
            output = self.variant_implementation(test_data)
            variant_outputs.append(output)
            print(f"    æœ€ç»ˆç»“æœ: {output[0, 0, 0].item():.8f}")
            print()
        
        print("ğŸ”§ æµ‹è¯•Invariantå®ç°ï¼ˆç¡®å®šæ€§åˆå¹¶ï¼‰:")
        invariant_outputs = []
        for i in range(num_tests):
            print(f"  ç¬¬{i+1}æ¬¡æ‰§è¡Œ:")
            output = self.invariant_implementation(test_data)
            invariant_outputs.append(output)
            print(f"    æœ€ç»ˆç»“æœ: {output[0, 0, 0].item():.8f}")
            print()
        
        # åˆ†æå·®å¼‚
        print("ğŸ“ˆ å·®å¼‚åˆ†æ:")
        
        # Variantå·®å¼‚
        variant_ref = variant_outputs[0]
        variant_diffs = []
        for output in variant_outputs[1:]:
            diff = torch.max(torch.abs(output - variant_ref)).item()
            variant_diffs.append(diff)
        
        print(f"  Variantæœ€å¤§å·®å¼‚: {max(variant_diffs):.2e}")
        print(f"  Variantå¹³å‡å·®å¼‚: {np.mean(variant_diffs):.2e}")
        
        # Invariantå·®å¼‚
        invariant_ref = invariant_outputs[0]
        invariant_diffs = []
        for output in invariant_outputs[1:]:
            diff = torch.max(torch.abs(output - invariant_ref)).item()
            invariant_diffs.append(diff)
        
        print(f"  Invariantæœ€å¤§å·®å¼‚: {max(invariant_diffs):.2e}")
        print(f"  Invariantå¹³å‡å·®å¼‚: {np.mean(invariant_diffs):.2e}")
        print()
        
        # åˆ†æåŸå› 
        print("ğŸ” åŸå› åˆ†æ:")
        if max(variant_diffs) < 1e-10:
            print("  âœ… Variantå®ç°å®é™…ä¸Šæ˜¯ç¡®å®šæ€§çš„")
            print("     å¯èƒ½åŸå› :")
            print("     1. çº¿ç¨‹æ± æ‰§è¡Œé¡ºåºç›¸å¯¹ç¨³å®š")
            print("     2. å°è§„æ¨¡æ•°æ®ï¼Œå¹¶è¡Œå¼€é”€å¤§äºæ”¶ç›Š")
            print("     3. ç³»ç»Ÿè°ƒåº¦ç›¸å¯¹ç¨³å®š")
        else:
            print("  âŒ Variantå®ç°ç¡®å®æ˜¯éç¡®å®šæ€§çš„")
        
        if max(invariant_diffs) < 1e-10:
            print("  âœ… Invariantå®ç°æ˜¯ç¡®å®šæ€§çš„")
        else:
            print("  âŒ Invariantå®ç°å‡ºç°éç¡®å®šæ€§")
            print("     å¯èƒ½åŸå› :")
            print("     1. çº¿ç¨‹ç«äº‰å¯¼è‡´è®¡ç®—é¡ºåºå˜åŒ–")
            print("     2. æµ®ç‚¹è¿ç®—çš„å¾®å°å·®å¼‚")
            print("     3. å†…å­˜è®¿é—®æ¨¡å¼çš„å½±å“")
    
    def test_larger_scale(self) -> None:
        """æµ‹è¯•æ›´å¤§è§„æ¨¡çš„æ•°æ®"""
        print("\n=== æµ‹è¯•æ›´å¤§è§„æ¨¡æ•°æ® ===\n")
        
        # åˆ›å»ºæ›´å¤§è§„æ¨¡çš„æµ‹è¯•æ•°æ®
        test_data = self.create_test_data(batch_size=4, seq_len=256, hidden_dim=512)
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
        
        # æµ‹è¯•æ¬¡æ•°
        num_tests = 10
        
        print("ğŸ”§ æµ‹è¯•Variantå®ç°:")
        variant_outputs = []
        for i in range(num_tests):
            output = self.variant_implementation(test_data, chunk_size=64, num_threads=4)
            variant_outputs.append(output)
            if i < 3:  # åªæ˜¾ç¤ºå‰3æ¬¡çš„ç»“æœ
                print(f"  ç¬¬{i+1}æ¬¡ç»“æœ: {output[0, 0, 0].item():.8f}")
        
        print("ğŸ”§ æµ‹è¯•Invariantå®ç°:")
        invariant_outputs = []
        for i in range(num_tests):
            output = self.invariant_implementation(test_data, chunk_size=64, num_threads=4)
            invariant_outputs.append(output)
            if i < 3:  # åªæ˜¾ç¤ºå‰3æ¬¡çš„ç»“æœ
                print(f"  ç¬¬{i+1}æ¬¡ç»“æœ: {output[0, 0, 0].item():.8f}")
        
        # åˆ†æå·®å¼‚
        print("\nğŸ“ˆ å¤§è§„æ¨¡æ•°æ®å·®å¼‚åˆ†æ:")
        
        # Variantå·®å¼‚
        variant_ref = variant_outputs[0]
        variant_diffs = []
        for output in variant_outputs[1:]:
            diff = torch.max(torch.abs(output - variant_ref)).item()
            variant_diffs.append(diff)
        
        print(f"  Variantæœ€å¤§å·®å¼‚: {max(variant_diffs):.2e}")
        print(f"  Variantå¹³å‡å·®å¼‚: {np.mean(variant_diffs):.2e}")
        
        # Invariantå·®å¼‚
        invariant_ref = invariant_outputs[0]
        invariant_diffs = []
        for output in invariant_outputs[1:]:
            diff = torch.max(torch.abs(output - invariant_ref)).item()
            invariant_diffs.append(diff)
        
        print(f"  Invariantæœ€å¤§å·®å¼‚: {max(invariant_diffs):.2e}")
        print(f"  Invariantå¹³å‡å·®å¼‚: {np.mean(invariant_diffs):.2e}")
        
        # ç»“è®º
        print("\nğŸ¯ ç»“è®º:")
        if max(variant_diffs) > max(invariant_diffs):
            print("  âœ… å¤§è§„æ¨¡æ•°æ®ä¸‹ï¼ŒVariantç¡®å®æ¯”Invariantæ›´ä¸ç¨³å®š")
        elif max(variant_diffs) < max(invariant_diffs):
            print("  âš ï¸ å¤§è§„æ¨¡æ•°æ®ä¸‹ï¼ŒInvariantæ¯”Variantæ›´ä¸ç¨³å®š")
        else:
            print("  âœ… å¤§è§„æ¨¡æ•°æ®ä¸‹ï¼Œä¸¤è€…ç¨³å®šæ€§ç›¸å½“")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = VariantInvariantAnalyzer(device='cpu')
    
    # 1. åˆ†æå°è§„æ¨¡æ•°æ®çš„ç¡®å®šæ€§è¡Œä¸º
    analyzer.analyze_determinism()
    
    # 2. æµ‹è¯•å¤§è§„æ¨¡æ•°æ®
    analyzer.test_larger_scale()
    
    print("\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
